{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dir = \"../project2/reviews/test/pos/\"\n",
    "dir_to_use = pos_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = os.listdir(dir_to_use)\n",
    "pos_files = [pf for pf in pos_files if pf[0] != \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the\n",
    "label = 1 # positive data should be labeled as 1 @@@@\n",
    "y = [] \n",
    "files = []\n",
    "avg_vecs = []\n",
    "all_text = []\n",
    "complexity_scores = []\n",
    "ST = nltk.tokenize.sonority_sequencing.SyllableTokenizer()\n",
    "for i in range(len(pos_files)):\n",
    "    afile = open(pos_dir+pos_files[i])\n",
    "    review = \"\".join(afile.readlines())\n",
    "    review = re.sub(r'<[^>]*>', '', review)\n",
    "    tokens = nltk.word_tokenize(review.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    word_count = len(tokens) ## complexity\n",
    "    syll_count = sum([len(ST.tokenize(tok)) for tok in tokens]) ## complexity\n",
    "    sent_count = len(nltk.sent_tokenize(review)) ## complexity\n",
    "    score = 206.835 - 1.015 * (word_count / sent_count) - 84.66 * (syll_count / word_count) ## complexity\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    n = 0\n",
    "    feature = np.zeros((300,))\n",
    "    for word in tokens:\n",
    "        token = nlp(word)\n",
    "        if token.has_vector:\n",
    "            n += 1\n",
    "            feature += token.vector\n",
    "    if n != 0:\n",
    "        feature /= n\n",
    "    avg_vecs.append(feature)\n",
    "    review = \" \".join(tokens)\n",
    "    y.append(label)\n",
    "    complexity_scores.append(score)\n",
    "    files.append(pos_files[i])\n",
    "    all_text.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files),len(avg_vecs),len(all_text),len(y),len(complexity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_data = np.array(avg_vecs)\n",
    "vecs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y.shape = (y.shape[0],1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = np.array(complexity_scores)\n",
    "complexity.shape = (complexity.shape[0],1)\n",
    "complexity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data_to_save = np.concatenate([y,complexity,vecs_data],axis=1)\n",
    "inter_data_to_save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"pos_data_inter_test\",inter_data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### nothing below this is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3,3),max_features=10000,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_arr = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 436)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = np.concatenate([y,complexity,X_arr,vecs_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"pos_data\",data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['storyline', 'max', 'von', 'sydow', 'narration', 'hypnotizes', 'protagonist', 'audience', 'back', 'protagonist', 'young', 'american', 'ideologist', 'leopold', 'kessler', 'barr', 'arrived', 'germany', 'help', 'rebuilding', 'damaged', 'country', 'uncle', 'kessler', 'järegård', 'supplies', 'leopold', 'job', 'big', 'zentropa', 'train', 'corporation', 'soon', 'leopold', 'falls', 'love', 'katharina', 'hartmann', 'barbara', 'sukowa', 'daughter', 'zentropa', 'owner', 'max', 'hartmann', 'jørgen', 'reenberg', 'leopold', 'soon', 'finds', 'caught', 'web', 'corruption', 'taken', 'advantage', 'losing', 'ideology', 'forced', 'chose', 'pest', 'mesmerizing', 'manipulative', 'noirish', 'haunting', 'beautiful', 'ugly', 'immediate', 'grandiose', 'descriptions', 'come', 'mind', 'thinking', 'lars', 'von', 'trier', 'masterpiece', 'europa', 'final', 'chapter', 'europa', 'trilogy', 'usa', 'retitled', 'zentropa', 'audiences', 'would', 'confuse', 'agnieszka', 'holland', 'europa', 'europa', 'equally', 'wwii', 'drama', 'europa', 'trilogy', 'also', 'consists', 'forbrydelsens', 'element', 'epidemic', 'infamous', 'experiment', 'sold', 'tickets', 'danish', 'cinemas', 'trilogy', 'thematically', 'deals', 'hypnotism', 'loss', 'idealism', 'although', 'themes', 'trilogy', 'essential', 'visuals', 'europa', 'see', 'locomotive', 'moving', 'towards', 'us', 'unidentified', 'narrator', 'literally', 'hypnotizes', 'us', 'mental', 'count', 'ten', 'europa', 'ten', 'say', 'ten', 'metaphor', 'movies', 'ability', 'transport', 'us', 'subconscious', 'utilizes', 'strange', 'extremely', 'effective', 'visual', 'style', 'famous', 'russian', 'director', 'andrei', 'tarkovsky', 'trier', 'says', 'movie', 'occasionally', 'intertwined', 'red', 'form', 'blood', 'red', 'dress', 'etc', 'according', 'rumors', 'inspired', 'steven', 'spielberg', 'use', 'similar', 'effect', 'shindler', 'list', 'coincidentially', 'another', 'wwii', 'drama', 'furthermore', 'trier', 'uses', 'dutch', 'angels', 'reinvents', 'adding', 'separately', 'shot', 'layers', 'upon', 'layers', 'unlike', 'old', 'hollywood', 'movies', 'incorporated', 'economical', 'reasons', 'trier', 'uses', 'artistic', 'reasons', 'carefully', 'executed', 'visual', 'techniques', 'underline', 'hypnotized', 'universe', 'europa', 'real', 'europa', 'often', 'criticized', 'weighing', 'advanced', 'technique', 'plot', 'characters', 'hey', 'reviewers', 'criticized', 'stanley', 'kubrick', 'visual', 'masterpiece', 'space', 'odyssey', 'nowadays', 'holds', 'obligatory', 'place', 'also', 'gets', 'accused', 'historical', 'incorrectness', 'apparently', 'trier', 'assigns', 'nazis', 'werewolf', 'much', 'historical', 'significance', 'according', 'various', 'correct', 'fascinating', 'subject', 'try', 'yet', 'trier', 'purposes', 'neither', 'educational', 'portraying', 'history', 'accurately', 'europa', 'nightmare', 'leopold', 'kessler', 'hypnotized', 'therefore', 'universe', 'audience', 'encounters', 'distorted', 'reality', 'equally', 'shows', 'memory', 'deceives', 'us', 'accurate', 'reconstruction', 'lie', 'although', 'young', 'audiences', 'experience', 'europa', 'young', 'memories', 'wwii', 'collective', 'memory', 'various', 'bbc', 'documentaries', 'small', 'inaccuracies', 'actually', 'serve', 'purpose', 'inform', 'us', 'us', 'germany', 'leopolds', 'memory', 'three', 'europa', 'trilogy', 'chapters', 'portray', 'young', 'ideologists', 'noble', 'intentions', 'forced', 'corruption', 'losing', 'ideological', 'innocence', 'ambiguous', 'endings', 'forbrydelsens', 'element', 'europa', 'show', 'ideologists', 'getting', 'forever', 'caught', 'hypnotized', 'realities', 'shooting', 'europa', 'poland', 'lars', 'von', 'trier', 'niels', 'vørsel', 'extremely', 'interested', 'wwii', 'shows', 'packed', 'extremely', 'beautiful', 'shots', 'catching', 'atmosphere', 'great', 'example', 'old', 'polish', 'church', 'europa', 'shot', 'poland', 'primarily', 'economic', 'reasons', 'last', 'act', 'europa', 'space', 'odyssey', 'think', 'europa', 'receive', 'rightfully', 'deserved', 'place', 'method', 'twisting', 'old', 'clichés', 'visual', 'techniques', 'unique', 'strange', 'completely', 'different', 'anything', 'see', 'hollywood', 'nowadays', 'essential', 'movie', 'lars', 'von', 'trier', 'catalog', 'write', 'pure', 'commercial', 'speculation', 'would', 'catastrophic', 'right', 'trier', 'classics', 'forbrydelsens', 'element', 'riget', 'dogville', 'unique', 'experience', 'trier', 'cared', 'actors', 'manifesto', 'watch', 'count', 'ten']\n"
     ]
    }
   ],
   "source": [
    "afile = open(pos_dir+test_fname) # pos, so label as 1\n",
    "review = \"\".join(afile.readlines())\n",
    "review = re.sub(r'<[^>]*>', '', review)\n",
    "tokens = nltk.word_tokenize(review.lower())\n",
    "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
