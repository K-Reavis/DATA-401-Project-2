{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dir = \"./reviews/train/neg/\"\n",
    "dir_to_use = neg_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_files = os.listdir(dir_to_use)\n",
    "neg_files = [pf for pf in neg_files if pf[0] != \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the\n",
    "label = 0 # positive data should be labeled as 1 @@@@\n",
    "y = [] \n",
    "files = []\n",
    "avg_vecs = []\n",
    "all_text = []\n",
    "complexity_scores = []\n",
    "ST = nltk.tokenize.sonority_sequencing.SyllableTokenizer()\n",
    "for i in range(len(neg_files)):\n",
    "    afile = open(neg_dir+neg_files[i])\n",
    "    review = \"\".join(afile.readlines())\n",
    "    review = re.sub(r'<[^>]*>', '', review)\n",
    "    tokens = nltk.word_tokenize(review.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    word_count = len(tokens) ## complexity\n",
    "    syll_count = sum([len(ST.tokenize(tok)) for tok in tokens]) ## complexity\n",
    "    sent_count = len(nltk.sent_tokenize(review)) ## complexity\n",
    "    score = 206.835 - 1.015 * (word_count / sent_count) - 84.66 * (syll_count / word_count) ## complexity\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    n = 0\n",
    "    feature = np.zeros((300,))\n",
    "    for word in tokens:\n",
    "        token = nlp(word)\n",
    "        if token.has_vector:\n",
    "            n += 1\n",
    "            feature += token.vector\n",
    "    if n != 0:\n",
    "        feature /= n\n",
    "    avg_vecs.append(feature)\n",
    "    review = \" \".join(tokens)\n",
    "    y.append(label)\n",
    "    complexity_scores.append(score)\n",
    "    files.append(neg_files[i])\n",
    "    all_text.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files),len(avg_vecs),len(all_text),len(y),len(complexity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_data = np.array(avg_vecs)\n",
    "vecs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y.shape = (y.shape[0],1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = np.array(complexity_scores)\n",
    "complexity.shape = (complexity.shape[0],1)\n",
    "complexity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data_to_save = np.concatenate([y,complexity,vecs_data],axis=1)\n",
    "inter_data_to_save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"neg_data_inter\",inter_data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
